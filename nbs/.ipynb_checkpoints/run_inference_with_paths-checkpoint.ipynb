{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Run inference with paths**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution line   \n",
    "`python train.py eval_only --config configs/KITTITrackletsCarsHard.json --eval_epoch 28`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'provider'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6a6624880338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mprovider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtp8\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMODEL_tp8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'provider'"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import provider\n",
    "import copy\n",
    "import models.tp8 as MODEL_tp8\n",
    "from config import load_config, configGlobal, save_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config and model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Namespace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38db8b74714c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m FLAGS = Namespace(config='configs/KITTITrackletsCarsHard.json', \n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0meval_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'28'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0moperation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval_only'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mrefineICP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Namespace' is not defined"
     ]
    }
   ],
   "source": [
    "FLAGS = Namespace(config='configs/KITTITrackletsCarsHard.json', \n",
    "                  eval_epoch='28', \n",
    "                  its=30, \n",
    "                  operation='eval_only', \n",
    "                  refineICP=False, \n",
    "                  refineICPmethod='p2p', \n",
    "                  use_old_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:' + str(cfg.gpu_index)):\n",
    "            pcs1, pcs2, translations, rel_angles, pc1centers, pc2centers, pc1angles, pc2angles = MODEL.placeholder_inputs(cfg.training.batch_size, cfg.model.num_points)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "            # Note the global_step=batch parameter to minimize.\n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('hyperparameters/bn_decay', bn_decay)\n",
    "\n",
    "            # Get model and loss\n",
    "            end_points = MODEL.get_model(pcs1, pcs2, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pcs1, pcs2, translations, rel_angles, pc1centers, pc2centers, pc1angles, pc2angles, end_points)\n",
    "            tf.summary.scalar('losses/loss', loss)\n",
    "\n",
    "            #  correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n",
    "            #  accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(cfg.training.batch_size)\n",
    "            #  tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('hyperparameters/learning_rate', learning_rate)\n",
    "            if cfg.training.optimizer.optimizer == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=cfg.training.optimizer.momentum)\n",
    "            elif cfg.training.optimizer.optimizer == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            else:\n",
    "                assert False, \"Invalid optimizer\"\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        # Add summary writers\n",
    "        # Documentation for summary.FileWriter:\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/compat/v1/summary/FileWriter\n",
    "        # One line descriptions: Writes Summary protocol buffers to event files\n",
    "        # Params:\n",
    "        #   - logdir: (str) path to save the logs\n",
    "        #   - graph:  (tf.Graph) current Graph object\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(cfg.logging.logdir, 'train'), sess.graph)\n",
    "        val_writer = tf.summary.FileWriter(os.path.join(cfg.logging.logdir, 'val'))\n",
    "        val_writer_180 = tf.summary.FileWriter(os.path.join(cfg.logging.logdir, 'val_180'))\n",
    "\n",
    "        # Init variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        # sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pcs1': pcs1, 'pcs2': pcs2, 'translations': translations, 'rel_angles': rel_angles, 'is_training_pl': is_training_pl, 'pred_translations': end_points['pred_translations'], 'pred_remaining_angle_logits': end_points['pred_remaining_angle_logits'], 'pc1centers': pc1centers, 'pc2centers': pc2centers, 'pc1angles': pc1angles, 'pc2angles': pc2angles, 'pred_s1_pc1centers': end_points['pred_s1_pc1centers'], 'pred_s1_pc2centers': end_points['pred_s1_pc2centers'], 'pred_s2_pc1centers': end_points['pred_s2_pc1centers'], 'pred_s2_pc2centers': end_points['pred_s2_pc2centers'], 'pred_pc1angle_logits': end_points['pred_pc1angle_logits'], 'pred_pc2angle_logits': end_points['pred_pc2angle_logits'], 'loss': loss, 'train_op': train_op, 'merged': merged, 'step': batch}\n",
    "\n",
    "        start_epoch = 0\n",
    "        if eval_only:\n",
    "            model_to_load = cfg.logging.logdir\n",
    "            if eval_only_model_to_load is not None:\n",
    "                model_to_load = eval_only_model_to_load\n",
    "            if not FLAGS.use_old_results and not do_timings:\n",
    "                assert os.path.isfile(f'{model_to_load}/model-{eval_epoch}.index'), f'{model_to_load}/model-{eval_epoch}.index'\n",
    "                saver.restore(sess, f'{model_to_load}/model-{eval_epoch}')\n",
    "            start_epoch = int(eval_epoch)\n",
    "\n",
    "            if eval_only_model_to_load is None:\n",
    "                num_batches_per_epoch = len(TRAIN_INDICES) // cfg.training.batch_size\n",
    "\n",
    "                if FLAGS.use_old_results or do_timings:\n",
    "                    start_epoch = int(eval_epoch)\n",
    "                else:\n",
    "                    restored_batch = sess.run(batch)\n",
    "                    assert restored_batch % num_batches_per_epoch == 0\n",
    "                    start_epoch = restored_batch // num_batches_per_epoch - 1\n",
    "                    assert start_epoch == int(eval_epoch)\n",
    "            logger.info(f'Evaluating at epoch {start_epoch}')\n",
    "        else:\n",
    "            if os.path.isfile(os.path.join(cfg.logging.logdir, 'model.ckpt.index')):\n",
    "                saver.restore(sess, os.path.join(cfg.logging.logdir, 'model.ckpt'))\n",
    "\n",
    "                num_batches_per_epoch = len(TRAIN_INDICES) // cfg.training.batch_size\n",
    "\n",
    "                restored_batch = sess.run(batch)\n",
    "                assert restored_batch % num_batches_per_epoch == 0\n",
    "                start_epoch = restored_batch // num_batches_per_epoch\n",
    "                logger.info(f'Continuing training at epoch {start_epoch}')\n",
    "            elif cfg.training.pretraining.model != '':\n",
    "                assert os.path.isfile(cfg.training.pretraining.model + '.index')\n",
    "                variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "                variables_to_load = [var for var in variables if var not in [batch]]\n",
    "                saverPretraining = tf.train.Saver(variables_to_load)\n",
    "                saverPretraining.restore(sess, cfg.training.pretraining.model)\n",
    "                #  print(variables)\n",
    "                #  print(len(variables), len(variables_to_load))\n",
    "                #  varlist = print_tensors_in_checkpoint_file(file_name=cfg.training.pretraining.model, all_tensors=True, tensor_name=None)\n",
    "                #  print(varlist)\n",
    "                #  print(variables_to_load[:8])\n",
    "                #  print(len(varlist))\n",
    "                restored_batch = sess.run(batch)\n",
    "                assert restored_batch == 0\n",
    "                logger.info(f'Pre-trained weights loaded from {cfg.training.pretraining.model}, starting initial evaluation')\n",
    "                lr, bn_d = sess.run([learning_rate, bn_decay])\n",
    "                eval_one_epoch(sess, ops, val_writer, val_writer_180, 'pretr', eval_only=False, do_timings=False)\n",
    "                logger.info(f'Initial evaluation finished')\n",
    "\n",
    "        try:\n",
    "            start = time.time()\n",
    "            for epoch in range(start_epoch, cfg.training.num_epochs):\n",
    "                lr, bn_d = sess.run([learning_rate, bn_decay])\n",
    "                logger.info('**** EPOCH %03d ****    ' % (epoch) + f'lr: {lr:.8f}, bn_decay: {bn_d:.8f}')\n",
    "                #  sys.stdout.flush()\n",
    "\n",
    "                if not eval_only:\n",
    "                    train_one_epoch(sess, ops, train_writer, epoch)\n",
    "                if eval_only or True or epoch % 10 == 0: # What is going on here?\n",
    "                    if do_timings:\n",
    "                        for i in range(10):\n",
    "                            eval_one_epoch(sess, ops, val_writer, val_writer_180, epoch, eval_only=eval_only, do_timings=True, override_batch_size=override_batch_size)\n",
    "                    else:\n",
    "                        eval_one_epoch(sess, ops, val_writer, val_writer_180, epoch, eval_only=eval_only, do_timings=False)\n",
    "                if eval_only:\n",
    "                    break\n",
    "\n",
    "                if not eval_only:\n",
    "                    was_last_epoch = epoch == cfg.training.num_epochs - 1\n",
    "                    # Save the variables to disk.\n",
    "                    if epoch % 2 == 0 or was_last_epoch:\n",
    "                        save_path = saver.save(sess, os.path.join(cfg.logging.logdir, \"model.ckpt\"))\n",
    "                        logger.info(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                    if epoch % 5 == 0 or was_last_epoch or cfg.evaluation.save_every_epoch:\n",
    "                        save_path = saver.save(sess, os.path.join(cfg.logging.logdir, \"model\"), global_step=epoch)\n",
    "                        logger.info(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                now = time.time()\n",
    "                time_elapsed = now - start\n",
    "                time_elapsed_str = str(datetime.timedelta(seconds=time_elapsed))\n",
    "                time_remaining = time_elapsed / (epoch + 1) * (cfg.training.num_epochs - epoch - 1)\n",
    "                time_remaining_str = str(datetime.timedelta(seconds=time_remaining))\n",
    "                logger.info(f'Finished epoch {epoch}. Time elapsed: {time_elapsed_str}, Time remaining: {time_remaining_str}')\n",
    "            logger.info('Finished Training')\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info('Interrupted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
