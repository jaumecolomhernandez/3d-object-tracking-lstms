{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "from scipy.spatial.transform import Rotation\n",
    "#from pointcloud import translate_transform_to_new_center_of_rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mat_angle(translation=None, rotation=None, rotation_center=np.array([0., 0, 0])):\n",
    "    mat1 = np.eye(4)\n",
    "    mat2 = np.eye(4)\n",
    "    mat3 = np.eye(4)\n",
    "    mat1[:3, 3] = -rotation_center\n",
    "    mat3[:3, 3] = rotation_center\n",
    "    if translation is not None:\n",
    "        mat3[:3, 3] += translation\n",
    "    if rotation is not None:\n",
    "        mat2[:3, :3] = Rotation.from_rotvec(np.array([0, 0, 1.]) * rotation).as_dcm()\n",
    "    return np.matmul(np.matmul(mat3, mat2), mat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_transform_to_new_center_of_rotation(all_pred_translations, all_pred_angles, all_pred_centers, all_gt_pc1centers):\n",
    "    new_all_pred_translations = np.zeros_like(all_pred_translations)\n",
    "    for idx, (pred_translation, pred_angle, pred_center, gt_pc1center) in enumerate(zip(all_pred_translations, all_pred_angles, all_pred_centers, all_gt_pc1centers)):\n",
    "        old_center, new_center = pred_center, gt_pc1center\n",
    "        center_shift = new_center - old_center\n",
    "        #  pred_transform = get_mat_angle(pred_translation, pred_angle, rotation_center=pred_center)\n",
    "        #  new_pred_translation = pred_translation\n",
    "        new_pred_translation = -center_shift + (np.matmul(get_mat_angle(rotation=pred_angle)[:3, :3], center_shift)) + pred_translation\n",
    "        new_all_pred_translations[idx] = new_pred_translation\n",
    "    return new_all_pred_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval_dir = '/home/usuario/project_data/trained/KITTITrackletsCars/val/eval000038'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gt_dir = '/home/usuario/project_data/datasets/KITTITrackletsCars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idxs = np.loadtxt(base_gt_dir+'split/val.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_arr(value):\n",
    "    \"\"\" Converts a string with float values to an actual list of floats \"\"\"\n",
    "    return [float(name) for name in value.split()]\n",
    "\n",
    "def fix_meta(meta):\n",
    "    \"\"\" Converts all the string coded values to lists \"\"\"\n",
    "    meta['start_position'] = to_arr(meta['start_position'])\n",
    "    meta['end_position'] = to_arr(meta['end_position'])\n",
    "    meta['translation'] = to_arr(meta['translation'])\n",
    "# Create dataset path\n",
    "def load_all_metas(dataset_path):\n",
    "    # Loads all jsons and stores them in a list(container)\n",
    "    container = list()\n",
    "    meta_path = os.path.join(base_gt_dir, \"meta\")\n",
    "    for filename in sorted(os.listdir(meta_path)):\n",
    "        # Create path and load file\n",
    "        file_path = os.path.join(meta_path, filename)\n",
    "        with open(file_path) as json_file: meta_dict = json.load(json_file)\n",
    "        # Convert the string to lists\n",
    "        fix_meta(meta_dict)\n",
    "        meta_dict['filename'] = filename[:-5]\n",
    "        # Append to file\n",
    "        container.append(meta_dict)\n",
    "    \n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "metas = load_all_metas(base_gt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt(eval_metas):\n",
    "    \"\"\"  \"\"\"\n",
    "    centers = np.empty((len(val_idxs), 3), dtype=np.float32)\n",
    "    translations = np.empty((len(val_idxs), 3), dtype=np.float32)\n",
    "    angles = np.empty((len(val_idxs), 1), dtype=np.float32)\n",
    "\n",
    "    for i, obs in enumerate(eval_metas):\n",
    "        centers[i,:] = obs['start_position'] \n",
    "        translations[i,:] = obs['translation'] \n",
    "        angles[i,:] = obs['rel_angle']\n",
    "    return {'centers': centers, 'translations': translations, 'angles': angles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gt_dir = '/home/usuario/project_data/datasets/KITTITrackletsCars/'\n",
    "val_idxs = np.loadtxt(base_gt_dir+'split/val.txt', dtype=int)    # List with all the values\n",
    "# Load predictions\n",
    "all_pred_translations = np.load(f'{base_eval_dir}/pred_translations.npy')\n",
    "all_pred_angles = np.load(f'{base_eval_dir}/pred_angles.npy')\n",
    "all_pred_centers = np.load(f'{base_eval_dir}/pred_s2_pc1centers.npy')\n",
    "\n",
    "# Load ground truth\n",
    "gt = get_gt(eval_metas)  \n",
    "all_gt_pc1centers = gt['centers']\n",
    "all_gt_translations = gt['translations']\n",
    "all_gt_angles = gt['angles']\n",
    "\n",
    "eval_dir=\"./\"\n",
    "accept_inverted_angle=False\n",
    "detailed_eval=False\n",
    "avg_window=5\n",
    "mean_time=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ns_to_dict(ns):\n",
    "    return {k: ns_to_dict(v) if type(v) == Namespace else v for k, v in ns.__dict__.items()}\n",
    "\n",
    "\n",
    "def eval_translation(t, gt_t):\n",
    "    levels = np.array([0, 0, 0])\n",
    "    level_thresholds = np.array([0.02, 0.1, 0.2])\n",
    "    dist = np.linalg.norm(t[:2] - gt_t[:2])\n",
    "    for idx, thresh in enumerate(level_thresholds):\n",
    "        if dist < thresh:\n",
    "            levels[idx] = 1\n",
    "    return dist, levels\n",
    "\n",
    "\n",
    "def angle_diff(a, b):\n",
    "    d = b - a\n",
    "    return float((d + np.pi) % (np.pi * 2.0) - np.pi)\n",
    "\n",
    "\n",
    "def eval_angle(a, gt_a, accept_inverted_angle):\n",
    "    levels = np.array([0, 0, 0])\n",
    "    level_thresholds = np.array([1., 5.0, 10.0])\n",
    "    dist = np.abs(angle_diff(a, gt_a)) / np.pi * 180.\n",
    "    if accept_inverted_angle:\n",
    "        dist = np.minimum(dist, np.abs(angle_diff(a + np.pi, gt_a)) / np.pi * 180.)\n",
    "    for idx, thresh in enumerate(level_thresholds):\n",
    "        if dist < thresh:\n",
    "            levels[idx] = 1\n",
    "    return dist, levels\n",
    "\n",
    "\n",
    "def eval_transform(t, gt_t, a, gt_a, accept_inverted_angle):\n",
    "    _, levels_translation = eval_translation(t, gt_t)\n",
    "    _, levels_angle = eval_angle(a, gt_a, accept_inverted_angle=accept_inverted_angle)\n",
    "    return np.minimum(levels_translation, levels_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_at_dist_measures(eval_measures, dist):\n",
    "    return Namespace(\n",
    "        corr_levels=eval_measures[dist]['corr_levels'].tolist(),\n",
    "        corr_levels_translation=eval_measures[dist]['corr_levels_translation'].tolist(),\n",
    "        mean_dist_translation=eval_measures[dist]['mean_dist_translation'],\n",
    "        mean_sq_dist_translation=eval_measures[dist]['mean_sq_dist_translation'],\n",
    "        corr_levels_angles=eval_measures[dist]['corr_levels_angles'].tolist(),\n",
    "        mean_dist_angle=eval_measures[dist]['mean_dist_angle'],\n",
    "        mean_sq_dist_angle=eval_measures[dist]['mean_sq_dist_angle'],\n",
    "        num=eval_measures[dist]['num'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_pred_translations = translate_transform_to_new_center_of_rotation(all_pred_translations, all_pred_angles, all_pred_centers, all_gt_pc1centers)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# 0.INIT EMPTY CONTAINERS FOR THE MEASURES\n",
    "# Diferents tracks\n",
    "tracks = defaultdict(dict)\n",
    "\n",
    "# Base dict for the measures \n",
    "empty_dict = {'corr_levels_translation': np.array([0, 0, 0], dtype=float), 'corr_levels_angles': np.array([0, 0, 0], dtype=float), 'corr_levels': np.array([0, 0, 0], dtype=float), 'mean_dist_translation': 0.0, 'mean_sq_dist_translation': 0.0, 'mean_dist_angle': 0.0, 'mean_sq_dist_angle': 0.0, 'num': 0}\n",
    "\n",
    "# Big dict fot the measures\n",
    "eval_measures = {\n",
    "    'all': copy.deepcopy(empty_dict),\n",
    "    '5m': copy.deepcopy(empty_dict),\n",
    "    '10m': copy.deepcopy(empty_dict),\n",
    "    '15m': copy.deepcopy(empty_dict),\n",
    "    '20m': copy.deepcopy(empty_dict),\n",
    "    'val': {\n",
    "        'all': copy.deepcopy(empty_dict),\n",
    "        '5m': copy.deepcopy(empty_dict),\n",
    "        '10m': copy.deepcopy(empty_dict),\n",
    "        '15m': copy.deepcopy(empty_dict),\n",
    "        '20m': copy.deepcopy(empty_dict),\n",
    "    },\n",
    "    'test': {\n",
    "        'all': copy.deepcopy(empty_dict),\n",
    "        '5m': copy.deepcopy(empty_dict),\n",
    "        '10m': copy.deepcopy(empty_dict),\n",
    "        '15m': copy.deepcopy(empty_dict),\n",
    "        '20m': copy.deepcopy(empty_dict),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Check \n",
    "per_transform_info = []\n",
    "\n",
    "# 1.COMPUTE EVALUATIONS FOR EACH OBSERVATION\n",
    "for idx, val_idx, translation, gt_translation, pred_angle, gt_angle, gt_pc1center in zip([x for x in range(len(val_idxs))], val_idxs, new_all_pred_translations, all_gt_translations, all_pred_angles, all_gt_angles, all_gt_pc1centers):\n",
    "    \n",
    "    # Load JSON (what for?)\n",
    "    meta = json.load(open(f'{base_gt_dir}/meta/{str(val_idx).zfill(8)}.json', 'r'))\n",
    "\n",
    "    # Depending on the track is test or it isn't\n",
    "    is_test = 'trackids' in meta and meta['trackids'][0] in [2, 6, 7, 8, 10]\n",
    "        \n",
    "    # Translation evaluation\n",
    "    dist_transl, levels_transl = eval_translation(translation, gt_translation)\n",
    "    # Angle evaluation\n",
    "    dist_angle, levels_angle = eval_angle(pred_angle, gt_angle, accept_inverted_angle=accept_inverted_angle)\n",
    "    # What does this do?\n",
    "    levels = eval_transform(translation, gt_translation, pred_angle, gt_angle, accept_inverted_angle=accept_inverted_angle)\n",
    "    \n",
    "    # What are sets?\n",
    "    for _set in ['both', 'val', 'test']:\n",
    "        if dist_transl > 10000:\n",
    "            continue\n",
    "        node = eval_measures\n",
    "        if _set in ['val', 'test']:\n",
    "            node = eval_measures[_set]\n",
    "            if (_set == 'test') != is_test:\n",
    "                continue\n",
    "        for key in ['all', '5m', '10m', '15m', '20m']:\n",
    "            centroid_distance = np.linalg.norm(gt_pc1center)\n",
    "            if key == '5m' and centroid_distance > 5.:\n",
    "                continue\n",
    "            if key == '10m' and centroid_distance > 10.:\n",
    "                continue\n",
    "            if key == '15m' and centroid_distance > 15.:\n",
    "                continue\n",
    "            if key == '20m' and centroid_distance > 20.:\n",
    "                continue\n",
    "            node[key]['num'] += 1\n",
    "            node[key]['corr_levels_translation'] += levels_transl\n",
    "            node[key]['mean_dist_translation'] += dist_transl\n",
    "            node[key]['mean_sq_dist_translation'] += dist_transl * dist_transl\n",
    "\n",
    "            node[key]['corr_levels_angles'] += levels_angle\n",
    "            node[key]['mean_dist_angle'] += dist_angle\n",
    "            node[key]['mean_sq_dist_angle'] += dist_angle * dist_angle\n",
    "\n",
    "            node[key]['corr_levels'] += levels\n",
    "\n",
    "    if detailed_eval:\n",
    "        per_transform_info.append([levels, dist_transl, dist_angle])\n",
    "\n",
    "\n",
    "for _set in ['both', 'val', 'test']:\n",
    "    node = eval_measures\n",
    "    if _set in ['val', 'test']:\n",
    "        node = eval_measures[_set]\n",
    "    for key in ['all', '5m', '10m', '15m', '20m']:\n",
    "        num_predictions = float(node[key]['num'])\n",
    "        if node[key]['num'] == 0:\n",
    "            num_predictions = 1e-20  # make numbers really large, indicates eval is not valid\n",
    "        node[key]['corr_levels_translation'] /= num_predictions\n",
    "        node[key]['mean_dist_translation'] /= num_predictions\n",
    "        node[key]['mean_sq_dist_translation'] = np.sqrt(node[key]['mean_sq_dist_translation'] / num_predictions)\n",
    "        node[key]['corr_levels_angles'] /= num_predictions\n",
    "        node[key]['mean_dist_angle'] /= num_predictions\n",
    "        node[key]['mean_sq_dist_angle'] = np.sqrt(node[key]['mean_sq_dist_angle'] / num_predictions)\n",
    "        node[key]['corr_levels'] /= num_predictions\n",
    "\n",
    "reg_eval_measures = np.array([0, 0], dtype=float)\n",
    "for idx, file_idx in enumerate(val_idxs):\n",
    "    meta = json.load(open(f'{base_gt_dir}/meta/{str(file_idx).zfill(8)}.json', 'r'))\n",
    "    if 'seq' in meta:\n",
    "        seq = meta['seq']\n",
    "        trackid = meta['trackids'][0]\n",
    "        frame1, frame2 = meta['frames']\n",
    "        intermediate_trackid = seq * 10000000 + trackid * 10000\n",
    "        pred_translation = all_pred_translations[idx]\n",
    "        time_passed = 0.1\n",
    "        tracks[intermediate_trackid][frame2] = (pred_translation, time_passed)\n",
    "\n",
    "#if len(tracks) > 0:\n",
    "#    velocities = process_velocities(tracks, eval_dir, avg_window)\n",
    "#    velocities\n",
    "#  print(velocities)\n",
    "\n",
    "eval_dict = Namespace(\n",
    "    corr_levels=eval_measures['all']['corr_levels'].tolist(),\n",
    "    corr_levels_translation=eval_measures['all']['corr_levels_translation'].tolist(),\n",
    "    mean_dist_translation=eval_measures['all']['mean_dist_translation'],\n",
    "    mean_sq_dist_translation=eval_measures['all']['mean_sq_dist_translation'],\n",
    "    corr_levels_angles=eval_measures['all']['corr_levels_angles'].tolist(),\n",
    "    mean_dist_angle=eval_measures['all']['mean_dist_angle'],\n",
    "    mean_sq_dist_angle=eval_measures['all']['mean_sq_dist_angle'],\n",
    "    num=eval_measures['all']['num'],\n",
    "    eval_5m=get_at_dist_measures(eval_measures, '5m'),\n",
    "    eval_10m=get_at_dist_measures(eval_measures, '10m'),\n",
    "    eval_15m=get_at_dist_measures(eval_measures, '15m'),\n",
    "    eval_20m=get_at_dist_measures(eval_measures, '20m'),\n",
    "    val=Namespace(\n",
    "        corr_levels=eval_measures['val']['all']['corr_levels'].tolist(),\n",
    "        corr_levels_translation=eval_measures['val']['all']['corr_levels_translation'].tolist(),\n",
    "        mean_dist_translation=eval_measures['val']['all']['mean_dist_translation'],\n",
    "        mean_sq_dist_translation=eval_measures['val']['all']['mean_sq_dist_translation'],\n",
    "        corr_levels_angles=eval_measures['val']['all']['corr_levels_angles'].tolist(),\n",
    "        mean_dist_angle=eval_measures['val']['all']['mean_dist_angle'],\n",
    "        mean_sq_dist_angle=eval_measures['val']['all']['mean_sq_dist_angle'],\n",
    "        num=eval_measures['val']['all']['num'],\n",
    "        eval_5m=get_at_dist_measures(eval_measures['val'], '5m'),\n",
    "        eval_10m=get_at_dist_measures(eval_measures['val'], '10m'),\n",
    "        eval_15m=get_at_dist_measures(eval_measures['val'], '15m'),\n",
    "        eval_20m=get_at_dist_measures(eval_measures['val'], '20m'),\n",
    "    ),\n",
    "    test=Namespace(\n",
    "        corr_levels=eval_measures['test']['all']['corr_levels'].tolist(),\n",
    "        corr_levels_translation=eval_measures['test']['all']['corr_levels_translation'].tolist(),\n",
    "        mean_dist_translation=eval_measures['test']['all']['mean_dist_translation'],\n",
    "        mean_sq_dist_translation=eval_measures['test']['all']['mean_sq_dist_translation'],\n",
    "        corr_levels_angles=eval_measures['test']['all']['corr_levels_angles'].tolist(),\n",
    "        mean_dist_angle=eval_measures['test']['all']['mean_dist_angle'],\n",
    "        mean_sq_dist_angle=eval_measures['test']['all']['mean_sq_dist_angle'],\n",
    "        num=eval_measures['test']['all']['num'],\n",
    "        eval_5m=get_at_dist_measures(eval_measures['test'], '5m'),\n",
    "        eval_10m=get_at_dist_measures(eval_measures['test'], '10m'),\n",
    "        eval_15m=get_at_dist_measures(eval_measures['test'], '15m'),\n",
    "        eval_20m=get_at_dist_measures(eval_measures['test'], '20m'),\n",
    "    ),\n",
    "    reg_eval=Namespace(fitness=reg_eval_measures[0], inlier_rmse=reg_eval_measures[1]),\n",
    "    #  num=len(val_idxs),\n",
    "    mean_time=mean_time)\n",
    "\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "filename = f'{eval_dir}/eval{\"_180\" if accept_inverted_angle else \"\"}.json'\n",
    "if os.path.isfile(filename):\n",
    "    datestr_now = datetime.datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    copyfile(filename, f'{filename[:-5]}_{datestr_now}.json')\n",
    "    if mean_time == 0:\n",
    "        prev_eval_dict = json.load(open(filename, 'r'))\n",
    "        if 'mean_time' in prev_eval_dict:\n",
    "            eval_dict.__dict__['mean_time'] = prev_eval_dict['mean_time']\n",
    "\n",
    "    with open(filename, 'w') as fhandle:\n",
    "        json.dump(ns_to_dict(eval_dict), fhandle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(corr_levels=[0.06606566200215286, 0.5060548977395048, 0.7459634015069968], corr_levels_angles=[0.5670075349838536, 0.6922766415500539, 0.8203713670613563], corr_levels_translation=[0.08032831001076426, 0.58745963401507, 0.7672228202368138], eval_10m=Namespace(corr_levels=[0.13953488372093023, 0.8057455540355677, 0.957592339261286], corr_levels_angles=[0.7701778385772914, 0.8714090287277702, 0.9671682626538988], corr_levels_translation=[0.16552667578659372, 0.8864569083447332, 0.9671682626538988], mean_dist_angle=6.833718817428427, mean_dist_translation=0.06107330318220551, mean_sq_dist_angle=31.083310430242665, mean_sq_dist_translation=0.10073639638011792, num=731), eval_15m=Namespace(corr_levels=[0.13473520249221183, 0.8029595015576324, 0.9493769470404985], corr_levels_angles=[0.7538940809968847, 0.8753894080996885, 0.9641744548286605], corr_levels_translation=[0.16433021806853582, 0.8800623052959502, 0.955607476635514], mean_dist_angle=7.45575269529974, mean_dist_translation=0.06406926663648307, mean_sq_dist_angle=33.012828415212674, mean_sq_dist_translation=0.10735323238325287, num=1284), eval_20m=Namespace(corr_levels=[0.1205141938939475, 0.7836100696304231, 0.9400107123727905], corr_levels_angles=[0.7284413497589716, 0.8634172469201928, 0.9566148901981789], corr_levels_translation=[0.15104445634708089, 0.8628816282806642, 0.9512587038028923], mean_dist_angle=8.69669229057386, mean_dist_translation=0.06808230823192603, mean_sq_dist_angle=36.078245256012224, mean_sq_dist_translation=0.11547732588900145, num=1867), eval_5m=Namespace(corr_levels=[0.12972972972972974, 0.7621621621621621, 0.9567567567567568], corr_levels_angles=[0.8108108108108109, 0.9027027027027027, 0.9837837837837838], corr_levels_translation=[0.14594594594594595, 0.8054054054054054, 0.9675675675675676], mean_dist_angle=4.04419694861805, mean_dist_translation=0.06896262696214221, mean_sq_dist_angle=22.277126893060647, mean_sq_dist_translation=0.0984968984952521, num=185), mean_dist_angle=28.68589275814463, mean_dist_translation=0.19650516482373032, mean_sq_dist_angle=67.6297507865793, mean_sq_dist_translation=0.36553209160234634, num=7432)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
